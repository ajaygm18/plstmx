#!/usr/bin/env python3
"""
Test script to validate enhanced training configuration for PMC10963254 compliance
Tests Bayesian optimization, extended training, EEMD, and timeout configuration
"""

import logging
import time
import numpy as np
from typing import Dict, Any

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def test_enhanced_configuration():
    """Test enhanced configuration settings"""
    try:
        logger.info("Testing enhanced configuration...")
        
        # Test configuration imports
        from config.settings import (
            BAYESIAN_OPT_CONFIG, 
            EXTENDED_TRAINING_CONFIG, 
            TIMEOUT_CONFIG, 
            EEMD_CONFIG,
            PLSTM_CONFIG
        )
        
        logger.info("âœ“ Enhanced configuration imported successfully")
        
        # Validate Bayesian optimization configuration
        assert BAYESIAN_OPT_CONFIG['n_calls'] >= 200, "n_calls should be >= 200 for extended optimization"
        assert BAYESIAN_OPT_CONFIG['n_initial_points'] >= 20, "n_initial_points should be >= 20"
        assert BAYESIAN_OPT_CONFIG['timeout'] is None, "timeout should be None for extended training"
        logger.info(f"âœ“ Bayesian optimization config: {BAYESIAN_OPT_CONFIG['n_calls']} calls")
        
        # Validate extended training configuration
        assert EXTENDED_TRAINING_CONFIG['target_accuracy'] == 0.70, "Target accuracy should be 70%"
        assert EXTENDED_TRAINING_CONFIG['max_training_time'] is None, "Training time should be unlimited"
        assert EXTENDED_TRAINING_CONFIG['checkpoint_interval'] > 0, "Checkpoint interval should be set"
        logger.info(f"âœ“ Extended training config: target accuracy {EXTENDED_TRAINING_CONFIG['target_accuracy']:.1%}")
        
        # Validate timeout configuration
        for key, value in TIMEOUT_CONFIG.items():
            assert value is None or value == 0, f"Timeout {key} should be None or 0, got {value}"
        logger.info("âœ“ All timeouts disabled successfully")
        
        # Validate EEMD configuration
        assert EEMD_CONFIG['trials'] >= 200, "EEMD trials should be >= 200 for better decomposition"
        assert EEMD_CONFIG['noise_width'] <= 0.2, "Noise width should be optimized"
        logger.info(f"âœ“ EEMD config: {EEMD_CONFIG['trials']} trials, {EEMD_CONFIG['noise_width']} noise width")
        
        # Validate PLSTM configuration
        assert PLSTM_CONFIG['epochs'] >= 300, "Epochs should be >= 300 for extended training"
        assert PLSTM_CONFIG['patience'] >= 50, "Patience should be >= 50 for extended training"
        logger.info(f"âœ“ PLSTM config: {PLSTM_CONFIG['epochs']} epochs, patience {PLSTM_CONFIG['patience']}")
        
        return True
        
    except Exception as e:
        logger.error(f"Configuration test failed: {str(e)}")
        return False

def test_bayesian_optimization():
    """Test enhanced Bayesian optimization"""
    try:
        logger.info("Testing enhanced Bayesian optimization...")
        
        from utils.bayesian_optimization import BayesianOptimizer
        
        # Initialize optimizer
        optimizer = BayesianOptimizer()
        
        # Check enhanced search space
        assert len(optimizer.search_space) >= 10, "Search space should have at least 10 parameters"
        assert optimizer.target_accuracy == 0.70, "Target accuracy should be 70%"
        
        # Check parameter ranges
        param_names = [space.name for space in optimizer.search_space]
        expected_params = ['lstm_units', 'attention_units', 'dropout_rate', 'learning_rate', 
                          'batch_size', 'sequence_length', 'epochs', 'recurrent_dropout']
        for param in expected_params:
            assert param in param_names, f"Parameter {param} missing from search space"
        
        logger.info(f"âœ“ Enhanced search space with {len(optimizer.search_space)} parameters")
        logger.info(f"âœ“ Target accuracy: {optimizer.target_accuracy:.1%}")
        
        return True
        
    except Exception as e:
        logger.error(f"Bayesian optimization test failed: {str(e)}")
        return False

def test_eemd_enhancement():
    """Test enhanced EEMD implementation"""
    try:
        logger.info("Testing enhanced EEMD implementation...")
        
        from utils.eemd_decomposition import EEMDDecomposer
        
        # Create test data
        np.random.seed(42)
        test_data = np.random.randn(1000) + np.sin(np.linspace(0, 10*np.pi, 1000))
        
        # Initialize enhanced EEMD
        decomposer = EEMDDecomposer()
        
        # Check enhanced configuration
        assert decomposer.trials >= 200, "EEMD trials should be >= 200"
        assert decomposer.noise_width <= 0.2, "Noise width should be optimized"
        
        logger.info(f"âœ“ Enhanced EEMD: {decomposer.trials} trials, {decomposer.noise_width} noise width")
        
        return True
        
    except Exception as e:
        logger.error(f"EEMD test failed: {str(e)}")
        return False

def test_plstm_enhanced_training():
    """Test enhanced PLSTM training configuration"""
    try:
        logger.info("Testing enhanced PLSTM training...")
        
        from models.plstm_tal import PLSTMTAL
        from config.settings import EXTENDED_TRAINING_CONFIG
        
        # Create mock training data
        np.random.seed(42)
        sequence_length = 60
        n_features = 32
        n_samples = 100
        
        X = np.random.randn(n_samples, sequence_length, n_features)
        y = np.random.randint(0, 2, (n_samples, 1)).astype(np.float32)
        
        # Initialize model
        model = PLSTMTAL(
            sequence_length=sequence_length,
            n_features=n_features,
            lstm_units=64,  # Smaller for testing
            attention_units=32,
            dropout_rate=0.2
        )
        
        # Build model
        model.build_model()
        
        # Test enhanced training parameters
        assert EXTENDED_TRAINING_CONFIG['target_accuracy'] == 0.70, "Target accuracy should be 70%"
        assert EXTENDED_TRAINING_CONFIG['max_training_time'] is None, "Training time should be unlimited"
        
        logger.info("âœ“ PLSTM model created with enhanced configuration")
        logger.info(f"âœ“ Target accuracy: {EXTENDED_TRAINING_CONFIG['target_accuracy']:.1%}")
        
        # Test that model can be trained (quick test with 1 epoch)
        if hasattr(model, 'model') and model.model is not None:
            logger.info("âœ“ Model architecture built successfully")
        
        return True
        
    except Exception as e:
        logger.error(f"PLSTM enhanced training test failed: {str(e)}")
        return False

def test_timeout_configuration():
    """Test that timeouts are properly disabled"""
    try:
        logger.info("Testing timeout configuration...")
        
        from config.settings import TIMEOUT_CONFIG
        
        # Check all timeout configurations
        timeout_checks = [
            ('training_timeout', TIMEOUT_CONFIG['training_timeout']),
            ('optimization_timeout', TIMEOUT_CONFIG['optimization_timeout']),
            ('evaluation_timeout', TIMEOUT_CONFIG['evaluation_timeout']),
            ('max_execution_time', TIMEOUT_CONFIG['max_execution_time'])
        ]
        
        for name, value in timeout_checks:
            assert value is None, f"{name} should be None, got {value}"
            logger.info(f"âœ“ {name}: disabled")
        
        logger.info("âœ“ All timeouts properly disabled for extended training")
        
        return True
        
    except Exception as e:
        logger.error(f"Timeout configuration test failed: {str(e)}")
        return False

def run_comprehensive_test():
    """Run comprehensive test of all enhancements"""
    logger.info("ðŸš€ Starting comprehensive test of PMC10963254 enhancements")
    logger.info("=" * 70)
    
    tests = [
        ("Enhanced Configuration", test_enhanced_configuration),
        ("Bayesian Optimization", test_bayesian_optimization),
        ("EEMD Enhancement", test_eemd_enhancement),
        ("PLSTM Enhanced Training", test_plstm_enhanced_training),
        ("Timeout Configuration", test_timeout_configuration)
    ]
    
    results = {}
    start_time = time.time()
    
    for test_name, test_func in tests:
        logger.info(f"\nðŸ“‹ Running: {test_name}")
        logger.info("-" * 50)
        
        try:
            result = test_func()
            results[test_name] = result
            status = "âœ… PASSED" if result else "âŒ FAILED"
            logger.info(f"{status}: {test_name}")
            
        except Exception as e:
            results[test_name] = False
            logger.error(f"âŒ FAILED: {test_name} - {str(e)}")
    
    total_time = time.time() - start_time
    
    # Summary
    logger.info("\n" + "=" * 70)
    logger.info("ðŸ“Š TEST SUMMARY")
    logger.info("=" * 70)
    
    passed = sum(results.values())
    total = len(results)
    
    for test_name, result in results.items():
        status = "âœ… PASSED" if result else "âŒ FAILED"
        logger.info(f"{status}: {test_name}")
    
    logger.info(f"\nResults: {passed}/{total} tests passed")
    logger.info(f"Total test time: {total_time:.2f}s")
    
    if passed == total:
        logger.info("ðŸŽ‰ ALL TESTS PASSED! Enhanced configuration ready for PMC10963254 compliance")
        logger.info("Ready for extended training with 70%+ accuracy target")
    else:
        logger.warning(f"âš ï¸  {total - passed} tests failed. Please review configuration.")
    
    return passed == total

if __name__ == "__main__":
    success = run_comprehensive_test()
    exit(0 if success else 1)